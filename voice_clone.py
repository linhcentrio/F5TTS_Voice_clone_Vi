# voice_clone.py - Phi√™n b·∫£n si√™u t·ªëi ∆∞u 2.0
import os
import sys
import warnings
from pathlib import Path
import streamlit as st

# B·ªè qua t·∫•t c·∫£ c·∫£nh b√°o kh√¥ng c·∫ßn thi·∫øt
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", message="Torchaudio's I/O functions")
warnings.filterwarnings("ignore", message="local_dir_use_symlinks")

# T·∫Øt t√≠nh nƒÉng theo d√µi c·ªßa Streamlit
os.environ.update({
    'STREAMLIT_SERVER_RUN_ON_SAVE': 'false',
    "STREAMLIT_WATCHER_WARNING_DISABLED": "true",
    "STREAMLIT_SERVER_WATCHER_TYPE": "none",
    "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:128"
})

# Kh·∫Øc ph·ª•c event loop Windows
if sys.platform.startswith('win'):
    try:
        import nest_asyncio
        nest_asyncio.apply()
    except ImportError:
        pass
    
    # Monkey patch ƒë·ªÉ ngƒÉn l·ªói torch._classes.__path__._path
    try:
        class PathProtector:
            def __getattr__(self, name):
                if name == '_path':
                    return []
                raise AttributeError(f"'{self.__class__.__name__}' has no attribute '{name}'")
                
        import types
        import torch
        if hasattr(torch, '_classes') and hasattr(torch._classes, '__path__'):
            torch._classes.__path__ = PathProtector()
    except:
        pass

# ƒê∆∞·ªùng d·∫´n
DIRS = {d: Path(f"./{d}").resolve() for d in ["model", "output_audio", "audio_ref", "temp_audio"]}
for d in DIRS.values():
    d.mkdir(parents=True, exist_ok=True)

# Import c√°c module
from utils.text_utils import setup_ttsnorm
from utils.audio_utils import generate_filename, save_audio
from utils.model_utils import load_tts_model, preload_vocoder
from components.ui import audio_source_selector, generation_parameters, audio_player, model_config_sidebar

# Thi·∫øt l·∫≠p TTSnorm
TTSnorm = setup_ttsnorm()

# C·∫•u h√¨nh UI
st.set_page_config(page_title="F5TTS Voice Clone", page_icon="üé§", layout="wide")
st.title("F5TTS - Nh√¢n B·∫£n Gi·ªçng N√≥i")

# T·∫£i vocoder cho Windows
if sys.platform.startswith('win'):
    vocos_path = preload_vocoder(DIRS["model"])
else:
    vocos_path = None

# Sidebar
model_config = model_config_sidebar(DIRS["model"])

# Tabs ch√≠nh
tab1, tab2 = st.tabs(["T·∫°o Gi·ªçng N√≥i", "C√†i ƒê·∫∑t N√¢ng Cao"])

with tab1:
    # Layout 2 c·ªôt
    col1, col2 = st.columns(2)
    
    with col1:
        # Ch·ªçn √¢m thanh tham chi·∫øu
        _, ref_audio_path = audio_source_selector(DIRS["audio_ref"], DIRS["temp_audio"])
        
        # VƒÉn b·∫£n tham chi·∫øu
        st.subheader("VƒÉn B·∫£n Tham Chi·∫øu")
        ref_text = st.text_area("VƒÉn b·∫£n tham chi·∫øu", height=100)
        clip_short = st.checkbox("C·∫Øt √¢m thanh d√†i", value=True)
    
    with col2:
        # VƒÉn b·∫£n ƒë·∫ßu v√†o
        st.header("VƒÉn B·∫£n C·∫ßn T·∫°o")
        input_text = st.text_area("VƒÉn b·∫£n c·∫ßn chuy·ªÉn th√†nh gi·ªçng n√≥i", height=200)
        
        # T·ªáp ƒë·∫ßu ra
        custom_filename = st.text_input("T√™n t·ªáp t√πy ch·ªânh (ƒë·ªÉ tr·ªëng = t·ª± ƒë·ªông)", value="")
        
        # N√∫t t·∫°o
        col1, col2 = st.columns(2)
        with col1:
            generate_button = st.button("üîä T·∫°o Gi·ªçng N√≥i", type="primary", use_container_width=True)
        with col2:
            quick_button = st.button("üöÄ T·∫°o Nhanh", use_container_width=True)

with tab2:
    # C√°c tham s·ªë n√¢ng cao
    params = generation_parameters()

# Container k·∫øt qu·∫£
result_container = st.container()

# T·∫£i m√¥ h√¨nh
@st.cache_resource
def get_model():
    return load_tts_model(
        model_config["model_path"], 
        model_config["vocab_file"], 
        model_config["vocoder_name"], 
        model_config["use_ema"],
        vocos_path
    )

tts_model, error = get_model()
model_loaded = tts_model is not None

if error:
    st.sidebar.error(error)
elif model_loaded:
    st.sidebar.success("‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o!")

# X·ª≠ l√Ω t·∫°o gi·ªçng n√≥i
def process_generation(input_text, ref_audio_path=None, ref_text=None, quick_mode=False):
    if not model_loaded:
        st.error("M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c t·∫£i")
        return
    
    if not input_text:
        st.error("Vui l√≤ng nh·∫≠p vƒÉn b·∫£n")
        return
    
    # Chu·∫©n h√≥a vƒÉn b·∫£n
    text_norm = TTSnorm(input_text)
    
    # T√™n t·ªáp ƒë·∫ßu ra
    output_filename = custom_filename if custom_filename else generate_filename(input_text, params["nfe_step"])
    output_path = str(DIRS["output_audio"] / output_filename)
    
    # X·ª≠ l√Ω tham chi·∫øu (n·∫øu kh√¥ng ph·∫£i ch·∫ø ƒë·ªô nhanh)
    if not quick_mode and ref_audio_path:
        with st.spinner("ƒêang x·ª≠ l√Ω √¢m thanh tham chi·∫øu..."):
            tts_model.preprocess_reference(
                ref_audio_path=ref_audio_path,
                ref_text=ref_text,
                clip_short=clip_short
            )
            
            # Hi·ªÉn th·ªã th·ªùi l∆∞·ª£ng
            ref_duration = tts_model.get_current_audio_length()
            st.info(f"Th·ªùi l∆∞·ª£ng tham chi·∫øu: {ref_duration:.2f} gi√¢y")
    
    # T·∫°o gi·ªçng n√≥i
    with st.spinner("ƒêang t·∫°o gi·ªçng n√≥i..."):
        import time
        start_time = time.time()
        
        # Tham s·ªë
        generate_params = {
            "text": text_norm,
            "output_path": output_path,
            "nfe_step": params["nfe_step"],
            "cfg_strength": params["cfg_strength"],
            "speed": params["speed"],
            "cross_fade_duration": params["cross_fade_duration"],
            "return_numpy": True
        }
        
        if params["show_spectrogram"]:
            generate_params["return_spectrogram"] = True
            result = tts_model.generate(**generate_params)
            audio_array, sample_rate, spectrogram = result
            
            # Hi·ªÉn th·ªã spectrogram
            import matplotlib.pyplot as plt
            fig, ax = plt.subplots(figsize=(10, 4))
            ax.imshow(spectrogram, origin="lower", aspect="auto")
            ax.set_title("Spectrogram")
            st.pyplot(fig)
        else:
            audio_array, sample_rate = tts_model.generate(**generate_params)
        
        generation_time = time.time() - start_time
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    audio_player(audio_array, sample_rate, output_filename, generation_time)
    
    # L∆∞u t·ªáp
    save_audio(audio_array, sample_rate, output_path)

# X·ª≠ l√Ω n√∫t t·∫°o
if generate_button and ref_audio_path:
    with result_container:
        process_generation(input_text, ref_audio_path, ref_text, quick_mode=False)
elif quick_button and input_text:
    with result_container:
        if not hasattr(tts_model, 'ref_audio_processed') or tts_model.ref_audio_processed is None:
            st.error("Ch∆∞a c√≥ tham chi·∫øu. Vui l√≤ng s·ª≠ d·ª•ng 'T·∫°o Gi·ªçng N√≥i' tr∆∞·ªõc.")
        else:
            process_generation(input_text, quick_mode=True)
elif generate_button and not ref_audio_path:
    st.error("Vui l√≤ng ch·ªçn √¢m thanh tham chi·∫øu")

# Gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ v·ªõi torch._classes.__path__._path trong Streamlit
def patch_streamlit_watcher():
    """Patch cho v·∫•n ƒë·ªÅ Streamlit v·ªõi torch._classes.__path__._path"""
    try:
        import streamlit.watcher.local_sources_watcher as watcher
        
        # L∆∞u h√†m extract_paths g·ªëc
        original_extract_paths = watcher.extract_paths
        
        # T·∫°o phi√™n b·∫£n an to√†n c·ªßa extract_paths
        def safe_extract_paths(module):
            try:
                # B·ªè qua c√°c module torch ƒë·ªÉ tr√°nh l·ªói
                if module.__name__.startswith('torch'):
                    return []
                return original_extract_paths(module)
            except Exception:
                return []
        
        # Thay th·∫ø h√†m extract_paths
        watcher.extract_paths = safe_extract_paths
    except:
        pass
