# components/ui.py
import streamlit as st
from pathlib import Path
from typing import Tuple, Dict, Any, Optional, List

def on_audio_recorded():
    """Callback khi ghi √¢m ho√†n t·∫•t."""
    if st.session_state.recorder_output:
        st.session_state.audio_recorded = True

def audio_source_selector(audio_ref_dir: Path, temp_dir: Path) -> Tuple[str, Optional[str]]:
    """Component ƒë·ªÉ ch·ªçn ngu·ªìn √¢m thanh tham chi·∫øu."""
    from utils.audio_utils import handle_audio_input, handle_recorded_audio
    from utils.model_utils import get_reference_audio_files
    
    st.header("√Çm Thanh Tham Chi·∫øu")
    
    # Ch·ªçn ngu·ªìn √¢m thanh
    audio_source = st.radio(
        "Ngu·ªìn √¢m thanh tham chi·∫øu",
        options=["T·∫£i t·ªáp l√™n", "Ghi √¢m tr·ª±c ti·∫øp", "Ch·ªçn t·ª´ th∆∞ m·ª•c"],
        horizontal=True
    )
    
    ref_audio_path = None
    
    if audio_source == "T·∫£i t·ªáp l√™n":
        uploaded_file = st.file_uploader(
            "T·∫£i l√™n √¢m thanh tham chi·∫øu", 
            type=["wav", "mp3", "flac", "ogg", "m4a"]
        )
        if uploaded_file:
            ref_audio_path = handle_audio_input(uploaded_file, temp_dir)
    
    elif audio_source == "Ghi √¢m tr·ª±c ti·∫øp":
        try:
            from streamlit_mic_recorder import mic_recorder
            
            # Kh·ªüi t·∫°o session state
            if "audio_recorded" not in st.session_state:
                st.session_state.audio_recorded = False
                
            st.info("**Ghi √¢m tham chi·∫øu** (khuy·∫øn ngh·ªã: 5-10 gi√¢y)")
            
            # Giao di·ªán ghi √¢m
            audio_data = mic_recorder(
                start_prompt="‚è∫Ô∏è B·∫Øt ƒë·∫ßu ghi √¢m",
                stop_prompt="‚èπÔ∏è D·ª´ng ghi √¢m",
                key="recorder",
                callback=on_audio_recorded
            )
            
            # X·ª≠ l√Ω khi ghi √¢m th√†nh c√¥ng
            if st.session_state.get("audio_recorded"):
                recorder_output = st.session_state.get("recorder_output")
                
                if recorder_output and "bytes" in recorder_output:
                    st.success(f"‚úÖ ƒê√£ ghi √¢m th√†nh c√¥ng! (t·ª∑ l·ªá m·∫´u: {recorder_output.get('sample_rate', 'N/A')}Hz)")
                    
                    # L∆∞u √¢m thanh v√† l·∫•y ƒë∆∞·ªùng d·∫´n
                    ref_audio_path = handle_recorded_audio(recorder_output, temp_dir)
                    
                    # N√∫t qu·∫£n l√Ω √¢m thanh
                    col1, col2 = st.columns(2)
                    with col1:
                        if st.button("üíæ L∆∞u v√†o th∆∞ m·ª•c"):
                            try:
                                timestamp = time.strftime("%Y%m%d-%H%M%S")
                                save_path = audio_ref_dir / f"recording_{timestamp}.wav"
                                
                                with open(ref_audio_path, "rb") as f_in:
                                    with open(save_path, "wb") as f_out:
                                        f_out.write(f_in.read())
                                st.success(f"ƒê√£ l∆∞u √¢m thanh v√†o {save_path}")
                            except Exception as e:
                                st.error(f"L·ªói l∆∞u √¢m thanh: {e}")
                    
                    with col2:
                        if st.button("üóëÔ∏è X√≥a ghi √¢m"):
                            try:
                                if "recorder_output" in st.session_state:
                                    del st.session_state.recorder_output
                                st.session_state.audio_recorded = False
                                if ref_audio_path and Path(ref_audio_path).exists():
                                    Path(ref_audio_path).unlink(missing_ok=True)
                                ref_audio_path = None
                                st.rerun()
                            except Exception as e:
                                st.error(f"L·ªói x√≥a ghi √¢m: {e}")
        except ImportError:
            st.error("Th∆∞ vi·ªán streamlit-mic-recorder ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
            if st.button("C√†i ƒë·∫∑t streamlit-mic-recorder"):
                import subprocess, sys
                subprocess.check_call([sys.executable, "-m", "pip", "install", "streamlit-mic-recorder"])
                st.success("ƒê√£ c√†i ƒë·∫∑t th∆∞ vi·ªán. Vui l√≤ng kh·ªüi ƒë·ªông l·∫°i ·ª©ng d·ª•ng.")
                st.rerun()
    else:
        # Ch·ªçn t·ª´ th∆∞ m·ª•c
        reference_files = get_reference_audio_files(audio_ref_dir)
        if not reference_files:
            st.warning("Kh√¥ng t√¨m th·∫•y t·ªáp √¢m thanh trong th∆∞ m·ª•c ./audio_ref")
        else:
            selected_audio = st.selectbox(
                "Ch·ªçn √¢m thanh tham chi·∫øu",
                options=reference_files
            )
            if selected_audio:
                ref_audio_path = str(audio_ref_dir / selected_audio)
                with open(ref_audio_path, "rb") as file:
                    st.audio(file, format=f"audio/{selected_audio.split('.')[-1]}")
    
    return audio_source, ref_audio_path

def model_config_sidebar(model_dir: Path) -> Dict[str, Any]:
    """Component c·∫•u h√¨nh m√¥ h√¨nh trong sidebar."""
    from utils.model_utils import get_available_models
    
    st.sidebar.header("C·∫•u h√¨nh m√¥ h√¨nh")
    
    # Tr·∫°ng th√°i thi·∫øt b·ªã
    import torch
    device = "cuda" if torch.cuda.is_available() else "cpu"
    st.sidebar.info(f"Thi·∫øt b·ªã: {device.upper()}")
    
    if device == "cuda":
        st.sidebar.success(f"üöÄ GPU: {torch.cuda.get_device_name(0)}")
    
    # Ch·ªçn m√¥ h√¨nh
    available_models = get_available_models(model_dir)
    selected_model = st.sidebar.selectbox("Ch·ªçn m√¥ h√¨nh", options=available_models)
    model_path = str(model_dir / selected_model)
    
    # C·∫•u h√¨nh th√™m
    vocab_file = st.sidebar.text_input("T·ªáp t·ª´ v·ª±ng", value=str(model_dir / "vocab.txt"))
    vocoder_name = st.sidebar.selectbox("B·ªô t·∫°o gi·ªçng n√≥i", options=["vocos", "bigvgan"])
    use_ema = st.sidebar.checkbox("S·ª≠ d·ª•ng tr·ªçng s·ªë EMA", value=True)
    
    # Cache control
    st.sidebar.divider()
    
    # N√∫t l√†m m·ªõi
    if st.sidebar.button("üîÑ L√†m m·ªõi"):
        st.cache_data.clear()
        st.rerun()
        
    # N√∫t x√≥a cache
    if st.sidebar.button("X√≥a b·ªô nh·ªõ ƒë·ªám"):
        st.cache_data.clear()
        st.cache_resource.clear()
        st.sidebar.success("ƒê√£ x√≥a b·ªô nh·ªõ ƒë·ªám")
        st.rerun()
    
    return {
        "model_path": model_path,
        "vocab_file": vocab_file,
        "vocoder_name": vocoder_name,
        "use_ema": use_ema,
        "device": device
    }

def generation_parameters() -> Dict[str, Any]:
    """Component c√°c tham s·ªë t·∫°o gi·ªçng n√≥i."""
    col1, col2 = st.columns(2)
    
    with col1:
        st.header("Tham S·ªë T·∫°o Gi·ªçng N√≥i")
        nfe_step = st.slider(
            "S·ªë b∆∞·ªõc kh·ª≠ nhi·ªÖu",
            min_value=8,
            max_value=50,
            value=20,
            help="Gi√° tr·ªã cao h∆°n = ch·∫•t l∆∞·ª£ng t·ªët h∆°n nh∆∞ng ch·∫≠m h∆°n"
        )
        
        cfg_strength = st.slider(
            "ƒê·ªô m·∫°nh h∆∞·ªõng d·∫´n",
            min_value=0.5,
            max_value=5.0,
            value=2.0,
            step=0.1,
            help="Ki·ªÉm so√°t ƒë·ªô trung th√†nh v·ªõi vƒÉn b·∫£n"
        )
        
        speed = st.slider(
            "T·ªëc ƒë·ªô gi·ªçng n√≥i",
            min_value=0.5,
            max_value=2.0,
            value=1.0,
            step=0.1,
            help="ƒêi·ªÅu ch·ªânh t·ªëc ƒë·ªô n√≥i"
        )
    
    with col2:
        st.header("X·ª≠ L√Ω √Çm Thanh")
        cross_fade_duration = st.slider(
            "Th·ªùi gian cross-fade",
            min_value=0.0,
            max_value=0.5,
            value=0.15,
            step=0.01,
            help="Th·ªùi gian chuy·ªÉn ti·∫øp gi·ªØa c√°c ph√¢n ƒëo·∫°n"
        )
        
        show_spectrogram = st.checkbox(
            "Hi·ªÉn th·ªã spectrogram",
            value=True,
            help="Hi·ªÉn th·ªã bi·ªÉu di·ªÖn h√¨nh ·∫£nh c·ªßa √¢m thanh"
        )
        
        use_cache = st.checkbox(
            "L∆∞u k·∫øt qu·∫£ v√†o b·ªô nh·ªõ ƒë·ªám",
            value=True,
            help="TƒÉng t·ªëc khi t·∫°o l·∫°i c√πng m·ªôt vƒÉn b·∫£n"
        )
    
    return {
        "nfe_step": nfe_step,
        "cfg_strength": cfg_strength,
        "speed": speed,
        "cross_fade_duration": cross_fade_duration,
        "show_spectrogram": show_spectrogram,
        "use_cache": use_cache
    }

def audio_player(audio_array, sample_rate, output_filename, generation_time=None):
    """Component ph√°t √¢m thanh v·ªõi t·∫£i xu·ªëng."""
    from utils.audio_utils import play_audio_from_array
    
    st.subheader("√Çm Thanh ƒê√£ T·∫°o")
    
    # Ph√°t √¢m thanh
    audio_buffer = play_audio_from_array(audio_array, sample_rate)
    
    # Th√¥ng b√°o th√†nh c√¥ng
    if generation_time:
        st.success(f"‚úÖ T·∫°o gi·ªçng n√≥i th√†nh c√¥ng trong {generation_time:.2f} gi√¢y!")
    else:
        st.success("‚úÖ T·∫°o gi·ªçng n√≥i th√†nh c√¥ng!")
    
    # N√∫t t·∫£i xu·ªëng
    if audio_buffer:
        st.download_button(
            label="T·∫£i Xu·ªëng",
            data=audio_buffer,
            file_name=output_filename,
            mime="audio/wav"
        )
    
    return audio_buffer
